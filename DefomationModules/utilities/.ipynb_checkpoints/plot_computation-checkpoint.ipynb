{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n# Computations on IMODAL\n\nIn this tutorial we show how to perform computations on GPU with KeOps.\n\nKeOps, which stands for KErnelOPerationS, allow us to perform kernel matrix reductions.\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import relevant Python modules.\n\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\nsys.path.append(\"../\")\nimport time\n\nimport torch\n\nimport imodal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each deformation modules that needs kernel matrix reduction has a plain Pytorch and KeOps implementation.\n\nIn order to select once and for all the computation backend one can simply call\n\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imodal.Utilities.set_compute_backend('keops')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Available compute backend are **keops** and **torch**. All subsequent deformation modules that are created will use the specified computation backend.\n\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = 2\nN = 10000\nsigma = 1e-3\n\ntranslations = imodal.DeformationModules.Translations(d, N, sigma)\n\nprint(translations.backend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\"><h4>Warning</h4><p>Changing computation backend will not affect already created modules and these will have to be initialized again.</p></div>\n\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to explicitely set the compute backend for a deformation module using the **backend** keyword.\nThe varifold attachment also offers a KeOps implementation in 3D (but not in 2D yet).\n\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keops_translations = imodal.DeformationModules.Translations(d, N, sigma, backend='keops')\n\ntorch_translations = imodal.DeformationModules.Translations(d, N, sigma, backend='torch')\n\nattachment = imodal.Attachment.VarifoldAttachment(3, [1.], backend='keops')\n\nprint(keops_translations.backend)\nprint(torch_translations.backend)\nprint(attachment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving computation on GPUs can be done. As IMODAL is built on top of Pytorch, the device parameter can either be a string or a torch.device object.\n\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "positions = torch.randn(N, d)\nkeops_translations.manifold.fill_gd(positions)\nkeops_translations.to_(device='cuda')\n\ntorch_translations = imodal.DeformationModules.Translations(d, N, sigma, positions.to(device='cuda'), backend='torch')\n\nprint(keops_translations.device)\nprint(torch_translations.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>Note</h4><p>It is thus possible to select the GPU on which to perform computations by putting **device='cuda:x'** where **x** specify the GPU index (such as given by the **nvidia-smi** command.</p></div>\n\n<div class=\"alert alert-danger\"><h4>Warning</h4><p>For now, when choosing the second method to select the computing device, susequent manifold filling with a different device will lead in an degenerate states which will ultimately fail. It is thus best to use the **to_()** method.</p></div>\n\n<div class=\"alert alert-info\"><h4>Note</h4><p></p></div>\n The **to_()** method can also be used to change the dtype of the module tensors, in exacly the same way as with Pytorch.\n\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compare performance for \n\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "points = torch.randn(N, d, device='cuda')\n\nstart = time.perf_counter()\nkeops_translations(points)\nprint(\"KeOps backend on GPU, elapsed timed={}\".format(time.perf_counter() - start))\n\nstart = time.perf_counter()\ntorch_translations(points)\nprint(\"Pytorch backend on GPU, elapsed timed={}\".format(time.perf_counter() - start))\n\nimodal.Utilities.set_compute_backend('torch')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
